// If you wish to make an apple pie from scratch, you must first uber-calc the universe.

Rig:
  describe(rig_sectors_min, "Minimum sectors require to mine")

  describe(rig_storage_latency, "Random Access latency on the storage medium (honest)", secs)
  describe(rig_storage_parallelization, "Storage Units that can be accessed in parallel (honest)")
  describe(rig_storage_min, "Minimum storage required to mine", bytes)
  describe(rig_storage_min_tib, "Minimum storage required to mine", tib)
  describe(rig_storage_parallelization_min, "Minimum storage units that must be accessed in parallel in order to mine (honest)")
  describe(rig_storage_unit_max, "Maximum storage capacity for a storage unit", bytes)
  describe(rig_storage_unit_max_tib, "Maximum storage capacity for a storage unit", tib)

  describe(rig_malicious_cost_per_year, "Cost of the malicious computing rig per year")
  describe(rig_cores, "Number of CPU cores")
  describe(rig_snark_parallelization, "Number of SNARK proofs that can be generated in parallel")
  describe(rig_storage_read_mbs, "Sequential read speed (MB/s)")
  describe(rig_storage_read_mibs, "Sequential read speed (MiB/s)")

  describe(rig_gpu_cost, "Cost of a GPU", dollar)
  describe(rig_cpu_cost, "Cost of a CPU", dollar)
  describe(rig_gpu_cost_secs, "Cost of a GPU per second", dollar)
  describe(rig_cpu_core_cost, "Cost of a single CPU core", dollar)
  describe(rig_cpu_core_cost_secs, "Cost of a CPU core per second", dollar)
  describe(rig_cpu_lifetime_years, "Lifetime of a CPU", years)
  describe(rig_gpu_lifetime_years, "Lifetime of a GPU", years)

  rig_storage_read_mibs = rig_storage_read_mbs * mb_to_mib
  rig_storage_min = rig_storage_min_tib * b_in_tib
  rig_sectors_min = rig_storage_min / sector_size
  rig_storage_parallelization_min = epost_data_access / block_time

  rig_storage_unit_max = rig_storage_min / rig_storage_parallelization_min
  rig_storage_unit_max_tib = rig_storage_unit_max / b_in_tib 

  rig_gpu_cost_secs = rig_gpu_cost / (rig_gpu_lifetime_years * secs_in_year)
  rig_cpu_core_cost = rig_cpu_cost / rig_cores
  rig_cpu_core_cost_secs = rig_cpu_core_cost / (rig_cpu_lifetime_years * secs_in_year)

Crypto:
  describe(snark_size, "Size of a SNARK proof", bytes)
  describe(snark_constraint_time, "Time of proving a SNARK constraint", secs)
  describe(hashing_amax, "Factor of difference between stock CPU hashing and the fastest hashing")
  describe(merkle_tree_datahash_constraints, "Constraints for hashing two nodes in CommD")
  describe(merkle_tree_datahash_time, "Time to hash two nodes in CommD", secs)
  describe(merkle_tree_hash_constraints, "Constraints for hashing two nodes in CommR")
  describe(merkle_tree_hash_time, "Time to hash two nodes in CommR", secs)
  describe(column_leaf_hash_constraints, "Constraints for hashing a single row in a column", secs)
  describe(column_leaf_hash_time, "Time to compute the hash of a single row in a column", secs)

Utils:
  describe(cid_size, "Size of a CID", bytes)
  describe(secs_in_month, "Number of seconds in a month")
  describe(secs_in_year, "Number of seconds in a year")
  describe(secs_in_day, "Number of seconds in a day")
  describe(secs_in_min, "Number of seconds in a minute")
  describe(b_in_eib, "Number of bytes in a EiB")
  describe(b_in_pib, "Number of bytes in a PiB")
  describe(b_in_tib, "Number of bytes in a TiB")
  describe(b_in_gib, "Number of bytes in a GiB")
  describe(b_in_mib, "Number of bytes in a MiB")
  describe(b_in_kib, "Number of bytes in a KiB")
  describe(gib_in_tib, "Number of GiB in a TiB")
  describe(gb_to_gib, "Ratio between GB and GiB")
  describe(mb_to_mib, "Ratio between MB and MiB")

  b_in_eib = (1024 * 1024 * 1024 * 1024 * 1024 * 1024)
  b_in_pib = (1024 * 1024 * 1024 * 1024 * 1024)
  b_in_tib = (1024 * 1024 * 1024 * 1024)
  b_in_gib = (1024 * 1024 * 1024)
  b_in_mib = (1024 * 1024)
  gib_in_tib = 1024
  b_in_kib = 1024
  secs_in_month = (24 * 60 * 60 * 30)
  secs_in_year = (24  * 60 * 60 * 365)
  secs_in_day = (24 * 60 * 60)
  secs_in_min = (60)
  cid_size = 32 + 1 + 1 + 1
  gb_to_gib = (1024 * 1024 * 1024) / (1000 * 1000 * 1000)
  mb_to_mib = (1024 * 1024) / (1000 * 1000)

ProofOfSpacetime:
  // post_vanilla_size = node_size * (log2(nodes) + 1) * post_challenges
  Security:
    describe(post_challenges, "Number of challenges in a PoSt proof")
    describe(post_lambda, "Bits of security of a PoSt proof")
    describe(post_soundness, "Probability of an adversary to succeed in generating a convincing PoSt proof")
    post_soundness = expt(2, -post_lambda)

  Size:
    describe(post_proof_size, "Size of a PoSt proof")
    post_proof_size = post_snark_size

  ProofGeneration:
    describe(post_time, "Time to generate a PoSt Proof on a single core CPU", secs)
    describe(post_time_parallel, "Parallel time to generate a PoSt proof on RIG_CORES CPU cores", secs)
    describe(post_snark_time, "Time to generate a PoSt SNARK", secs)
    describe(post_snark_time_parallel, "Time to generate a PoSt SNARK in parallel", secs)
    describe(post_proof_gen, "Time to generate a PoSt Proof (including SNARKs, generating witnesses and reading from storage the challenged leaves)", secs)
    describe(post_proof_gen_parallel, "Time to generate a PoSt Proof (including SNARKs, generating witnesses and reading from storage the challenged leaves) in parallel", secs)
    describe(post_inclusions_time, "Time to generate merkle tree inclusion proofs for PoSt (assuming all data in memory)", secs)
    describe(post_inclusions_time_parallel, "Time to regenerate and to read the merkle tree inclusion proofs for the challenged nodes from memory in parallel", secs)
    describe(post_ticket_gen, "Time to generate a ticket", secs)

    post_time = post_proof_gen
    post_time_parallel = post_proof_gen_parallel
    post_proof_gen = post_snark_time + post_inclusions_time + post_ticket_gen + post_data_access 
    // TODO: this should be max(), not sum
    post_proof_gen_parallel = post_snark_time_parallel + post_inclusions_time_parallel + post_data_access_parallel + post_ticket_gen

    // This only accounts for generating the merkle trees
    Ticket:
      describe(ticket_hash, "Time to compute the hash of a node for ticket generation in PoSt", secs)
      post_ticket_gen = ticket_hash * post_challenges

    InclusionProofs:
      Caching:
        describe(post_mtree_cached, "Size of the cached parts of the replica merkle tree", bytes)
        describe(post_mtree_cached_hashnodes, "Number of intermediary hash nodes cached of the replica merkle tree")
        describe(post_mtree_cached_gib, "POST_MTREE_CACHED in GiB")
        describe(post_mtree_overhead, "Fraction of extra storage a miner will have to use to cache the replica merkle tree")
        describe(post_mtree_layers_cached, "Number of layers of the replica merkle tree being cached")
        describe(post_mtree_layers_deleted, "Number of layers of the replica merkle tree not being cached")

        post_mtree_cached = post_mtree_cached_hashnodes * node_size
        post_mtree_cached_gib = post_mtree_cached / b_in_gib 
        post_mtree_overhead = post_mtree_cached / sector_size
        post_mtree_cached_hashnodes = expt(2, post_mtree_layers_cached) - 1
        post_mtree_layers_deleted = commr_tree_depth - post_mtree_layers_cached

      Generation:
        describe(post_mtree_regen_hashnodes, "Number of intermediary nodes to be regenerated for a single replica merkle tree inclusion proof in PoSt")
        describe(post_mtree_regen_nodes, "Number of leaf nodes to be access for a single replica merkle tree inclusion proof in PoSt")
        describe(post_inclusion_time, "Time to generate a Merkle tree inclusion proof for PoSt (assuming all data in memory)", secs)

        post_mtree_regen_hashnodes = expt(2, post_mtree_layers_deleted + 1) - 1 // TODO check if -1 should be removed

        post_mtree_regen_nodes = expt(2, post_mtree_layers_deleted) - 1 // TODO check if -1 should be removed
        post_inclusions_time = post_inclusion_time * post_challenges
        post_inclusions_time_parallel = post_inclusions_time / rig_cores

      Generation (Wrapping):
        post_inclusion_time = (post_mtree_regen_hashnodes - 1) * merkle_tree_hash_time

      Generation (StackedReplicas):
        post_inclusion_time = (post_mtree_regen_hashnodes - 1) * merkle_tree_hash_time

      Generation (WrappingVariant):
        describe(wrapper_kdf_time, "Time to generate a node on the wrapper layer", secs)

        wrapper_kdf_time = kdf_time * wrapper_parents_all * post_mtree_regen_nodes
        post_inclusion_time = (post_mtree_regen_hashnodes - 1) * merkle_tree_hash_time + wrapper_kdf_time

  StorageAccess:
    describe(post_mtree_read, "Time to read Merkle tree data during PoSt", secs)
    describe(post_mtree_read_parallel, "Time to read Merkle tree data during PoSt", secs)
    describe(post_challenge_read, "Time to read from storage medium data required for a single challenge in PoSt", secs)
    describe(post_data_access, "Time to retrieve from the storage unit the data for PoSt", secs)
    describe(post_data_access_parallel, "Time to retrieve from the storage unit the data in parallel for PoSt", secs)
    describe(post_leaf_read, "Time to retrieve the challenged leaf in PoSt", secs)
    describe(post_leaves_read, "Time to retrieve the all the challenged leaves in PoSt", secs)
    describe(post_leaves_read_parallel, "Time to retrieve the all the challenged leaves in PoSt in parallel", secs)

    post_data_access = post_challenge_read * post_challenges
    post_leaves_read = post_leaf_read * post_challenges
    // TODO(enhancement): calculate first mtree and then challenge_read
    post_mtree_read = (post_challenge_read - post_leaf_read) * post_challenges
    post_data_access_parallel = post_data_access / rig_storage_parallelization
    post_leaves_read_parallel = post_leaves_read / rig_storage_parallelization
    post_mtree_read_parallel = post_mtree_read / rig_storage_parallelization

    Access (Wrapping):
      // TODO(enhancement): this is using random access, we should use seq access instead
      post_leaf_read = rig_storage_latency
      post_challenge_read = (1 + post_mtree_regen_nodes) * rig_storage_latency
      // TODO: log2(nodes) > post_mtree_layers_cached

    Access (WrappingVariant):
      describe(wrapper_lookup_read, "Time to retrieve from storage medium all the parents of a wrapper layer node", secs)
      describe(wrapper_lookup_with_mtree, "Time to retrieved from storage medium all the parents of the challenged node and those required for the merkle tree regeneration", secs)

      wrapper_lookup_read = wrapper_parents_all * rig_storage_latency
      post_leaf_read = wrapper_lookup_read

      wrapper_lookup_with_mtree = wrapper_lookup_read * (1 + post_mtree_regen_nodes)
      post_challenge_read = wrapper_lookup_with_mtree 

    // TODO: this might be incorrect depending on the vector
    // TODO: fix vector calculation
    // TODO: add merkle tree reads
    Access (StackedReplicas):
      describe(stack_lookup_read, "Time to read from storage medium the column of a single window", secs)
      describe(rig_storage_random_access_size, "Size of storage read via a random access in the storage medium", bytes)
      describe(rig_storage_random_access_nodes, "Number of nodes read via a random access in the storage medium")


      // TODO: this should be a parameter, not hardcoded
      DataAligned (StackedReplicaAligned):
        rig_storage_random_access_size = 4 * b_in_kib
        rig_storage_random_access_nodes = rig_storage_random_access_size / node_size

        // The first random_access_nodes are read by the random read, the remaining ones are read sequentially
        stack_lookup_read = rig_storage_latency + ((windows - rig_storage_random_access_nodes) * node_size_mib) / rig_storage_read_mibs

      DataAligned (StackedReplicaUnaligned):
        stack_lookup_read = rig_storage_latency * windows

      post_leaf_read = stack_lookup_read 
      post_challenge_read = stack_lookup_read 

  SNARK:
    describe(inclusion_proof, "Constraints for checking an inclusion proof in a binary merkle tree as large as the sector")
    describe(post_inclusion_proofs, "Constraints for checking all inclusion proofs (in PoSt)")
    describe(ticket_proofs, "Constraints for checking the partial ticket (in PoSt)")
    describe(ticket_constraints, "Constraints of a partial ticket per challenged node (in PoSt)")
    describe(post_snark_constraints, "Constraints for a PoSt")
    describe(post_snark_partitions, "Number of partitions of a PoSt proof")
    describe(post_snark_size, "Size of a PoSt proof", bytes)
    describe(post_snark_size_kib, "Size of a Post proof", kib)
    describe(post_snark_partition_constraints, "Number of constraints for a PoSt partition")
    describe(post_snark_partition_time, "Time to generate a single PoSt SNARK partition", secs)
    describe(commr_column_size, "Number of nodes in the CommR leaf")

    // TODO: make sure VectorR/etc are handled correctly
    inclusion_proof = merkle_tree_hash_constraints * commr_tree_depth 
    post_inclusion_proofs = inclusion_proof * post_challenges
    ticket_proofs = ticket_constraints * commr_column_size * post_challenges
    // TODO: handle different constructions
    post_snark_constraints = ticket_proofs + post_inclusion_proofs
    post_snark_partitions = post_snark_constraints / post_snark_partition_constraints
    post_snark_size = post_snark_partitions * snark_size
    post_snark_size_kib = post_snark_size / b_in_kib
    post_snark_time = snark_constraint_time * post_snark_constraints
    post_snark_partition_time = snark_constraint_time * post_snark_constraints
    // Parallelization calculation: it must take at least the time for a single snark
    // the remaining can be done in parallel
    // Similar in spirit to:
    // min(porep_snark_partition_time, porep_snark_time / snarrk_parallelization)
    post_snark_time_parallel = post_snark_partition_time + (post_snark_time - post_snark_partition_time) / (rig_snark_parallelization)

// TODO: add proof of window possession
ProofOfReplication:
  Security:
    describe(porep_challenges, "Number of challenges in a PoRep proof")
    describe(porep_lambda, "Bits of security of a PoRep proof")
    describe(spacegap, "Maximum difference in storage between an honest prover and a malicious prover")
    describe(porep_soundness, "Probability of an adversary to succeed in generating a convincing PoRep proof")

    porep_soundness = expt(2, -porep_lambda)

  Graph:
    describe(nodes, "Number of nodes in a sector")
    describe(stacked_layers, "Number of stacked layers of the graph")

    Sector:
      describe(sector_size, "Size of a sector", bytes)
      describe(sector_size_mib, "Size of a sector", mib)
      describe(sector_size_gib, "Size of a sector", gib)

      sector_size = sector_size_gib * b_in_gib
      sector_size_mib = sector_size / b_in_mib 

    Window:
      describe(window_nodes, "Number of nodes in a window")
      describe(window_size, "Size of a window", bytes)
      describe(window_size_mib, "Size of a window", mib)
      describe(window_size_gib, "Size of a window", mib)
      describe(windows, "Number of windows in a sector")

      window_size = window_size_mib * b_in_mib 
      window_size = window_size_gib * b_in_gib 
      windows = sector_size / window_size
      window_nodes = nodes / windows

    Parents:
      describe(graph_parents, "Number of parents of a node in the PoRep window graph")
      describe(drg_parents, "Number of DRG parents of a node in the PoRep window graph")
      describe(expander_parents, "Number of Chung Expander parents of a node in the PoRep window graph")
      describe(wrapper_parents, "Number of parents in the wrapper layer per window")
      describe(wrapper_parents_all, "Number of parents in the wrapper layer")
      wrapper_parents_all = wrapper_parents * windows

      graph_parents = drg_parents + expander_parents

    Nodes:
      describe(node_size, "Size of a node", bytes)
      describe(node_size_mib, "Size of a node", mib)
      describe(node_size_gib, "Size of a node", gib)
      describe(kdf_hash_size, "Size of the input to the KDF hash", bytes)
      describe(kdf_hash_size_gib, "Size of the input to the KDF hash", gi)

      nodes = sector_size / node_size
      node_size_mib = node_size / b_in_mib 
      node_size_gib = node_size / b_in_gib 
      kdf_hash_size_gib = graph_parents * node_size_gib
      kdf_hash_size = kdf_hash_size_gib * b_in_gib

  Time:
    describe(porep_time, "Time to seal a sector (PoRep proof)", secs)
    describe(porep_time_mins, "Time to seal a sector (PoRep proof)", mins)
    describe(porep_time_parallel, "Time to seal a sector (PoRep proof) with parallelization", secs)
    describe(porep_proof_gen, "Time to generate a PoRep proof (SNARK, witness generation, loading data from storage medium)", secs)
    describe(porep_proof_gen_parallel, "Time to generate a PoRep proof (SNARK, witness generation, loading data from storage medium) in parallel", secs)

    porep_proof_gen = porep_snark_time // TODO(enhancement): add porep_inclusions_time + porep_data_access 
    porep_proof_gen_parallel = porep_snark_time_parallel

    porep_time = porep_proof_gen + porep_commit_time + encoding_time
    porep_time_mins = porep_time / secs_in_min
    porep_time_parallel = porep_proof_gen_parallel + porep_commit_time_parallel + encoding_time_parallel

  Size:
    describe(porep_proof_size, "Size of a PoRep proof")
    describe(porep_proof_size_kib, "Size of a PoRep proof", kib)
    porep_proof_size = porep_snark_size + seal_onchain_commitments_size
    porep_proof_size_kib = porep_proof_size / b_in_kib

  // TODO: rename commR to commRLast
  Commitment:
    describe(window_comm_tree_time, "Time to generate a merkle tree of stacked windows", secs)
    describe(window_comm_leaves_time, "Time to commit columns of a window", secs)

    describe(commit_size, "Size of a commitment", bytes)
    describe(commr_size, "Size of CommR", bytes)
    describe(commd_size, "Size of CommD", bytes)
    describe(commrlast_size, "Size of CommRLast", bytes)

    describe(commr_tree_depth, "Number of layers of the merkle tree with root hash CommR")

    describe(seal_onchain_commitments_size, "Size of the seal commitments", bytes)
    describe(porep_commit_time, "Time to generate CommC, CommR and CommQ", secs)
    describe(porep_commit_time_mins, "Time to generate CommC, CommR and CommQ", mins)
    describe(porep_commit_time_parallel, "Time to generate CommC, CommR and CommQ in parallel", secs)

    describe(commr_time, "Time to generate CommR", secs)
    describe(commq_time, "Time to generate CommQ", secs)
    describe(commd_time, "Time to generate CommD", secs)
    describe(commc_time, "Time to generate the merkle tree of CommC (assuming leaves in memory)", secs)

    commit_size = cid_size
    window_comm_tree_time = merkle_tree_hash_time * (window_nodes - 1)
    window_comm_leaves_time = column_leaf_hash_time * (window_nodes - 1)
    commr_size = commit_size
    commd_size = commit_size
    commrlast_size = commit_size

    seal_onchain_commitments_size = commd_size + commr_size
    // seal_commitment_time = commr_time

    commd_time = merkle_tree_datahash_time * (nodes - 1)

    commq_time = commr_time
    porep_commit_time = commr_time + commq_time + commc_time
    porep_commit_time_parallel = porep_commit_time / rig_cores 
    porep_commit_time_mins = porep_commit_time / 60

    CommR (Wrapping): // was VectorR
      commr_time = merkle_tree_hash_time * (nodes - 1)
      commr_tree_depth = log2(nodes) 
      commr_column_size = 1

    CommR (WrappingVariant): // was VectorR
      commr_time = merkle_tree_hash_time * (nodes - 1)
      commr_tree_depth = log2(nodes) 
      commr_column_size = 1

    CommR (StackedReplicas): // was ColumnR
      commr_time = merkle_tree_hash_time * (nodes - 1)
      commr_tree_depth = log2(nodes) 
      commr_column_size = 1
      // commr_time = window_comm_tree_time + window_comm_leaves_time 
      // commr_tree_depth = log2(window_nodes)
      // commr_column_size = windows

    CommC:
      describe(commc_tree_time, "Time to generate CommC (Merkle tree only)", secs)
      describe(commc_leaves_time, "Time to generate CommC (Leaves only)", secs)
      commc_tree_time = window_comm_tree_time
      commc_leaves_time = window_comm_leaves_time * stacked_layers * windows
      commc_time = commc_tree_time + commc_leaves_time

  Encoding:
    describe(encoding_time, "Time to encode a sector", secs)
    describe(encoding_time_parallel, "Time to encode a sector in parallel", secs)
    describe(encoding_time_asic, "Time to encode a sector with an ASIC", secs)
    describe(encoding_window_time, "Time to encode a window", secs)
    describe(encoding_window_time_parallel, "Time to encode a window in parallel", secs)
    describe(encoding_window_time_asic, "Time to encode a window with an ASIC", secs)
    describe(encoding_window_time_parallel_asic, "Time to encode a window in parallel with ASICs", secs)
    describe(kdf_time, "Time to compute a KDF", secs)

    // TODO: adding wrapper layer
    encoding_time = encoding_window_time * windows

    encoding_window_time = window_nodes * ((1 + graph_parents) * kdf_time) * stacked_layers

    ParallelEncoding (StackedSDRParameters):
      encoding_time_parallel = encoding_window_time + ((windows - 1) * encoding_window_time) / rig_cores
      encoding_window_time_parallel = encoding_window_time

    ParallelEncoding (StackedChungParameters):
      encoding_time_parallel = encoding_time / rig_cores
      encoding_window_time_parallel = encoding_window_time / rig_cores

    Fastest:
      describe(encoding_time_fastcpu, "Time to encode with the fastest stock CPU", secs)
      describe(encoding_time_mins_fastcpu, "Time to encode with the fastest stock CPU", mins)
      describe(kdf_time_fastcpu, "Time to do a 32 bytes hash on the fastest stock CPU", secs)
      encoding_time_fastcpu = windows * (window_nodes * ((1 + graph_parents) * kdf_time_fastcpu) * stacked_layers)

      encoding_time_mins_fastcpu = encoding_time_fastcpu / secs_in_min

    Asic:
      encoding_time_asic = encoding_time_fastcpu / hashing_amax
      encoding_window_time_asic = encoding_window_time / hashing_amax
      encoding_window_time_parallel_asic = encoding_window_time_parallel / hashing_amax

  Retrieval:
    describe(window_read_time, "Time to read a window from storage unit", secs)
    describe(window_read_time_parallel, "Time to read a window from storage unit in parallel", secs)
    describe(decoding_time, "Time to decode a sector", secs)
    describe(decoding_time_parallel, "Time to decode a sector in parallel", secs)
    describe(decoding_first_node_time, "Time to decode the first node in a sector", secs)
    describe(decoding_first_node_time_parallel, "Time to decode the first node in a sector in parallel", secs)

    // This scales down encoding time by one layer
    Decoding (StackedReplicas):
      // Decoding in StackedReplicas: read the window output and decode it
      window_read_time = window_size_mib / rig_storage_read_mibs
      window_read_time_parallel = window_read_time / rig_storage_parallelization
      decoding_time = encoding_window_time + window_read_time

      // Note: this is an upperbound, it can be lower in theory min(time, disk access)
      decoding_time_parallel = encoding_window_time_parallel + window_read_time_parallel

      decoding_first_node_time = encoding_window_time * ((stacked_layers - 1) / stacked_layers)
      decoding_first_node_time_parallel = encoding_window_time_parallel * ((stacked_layers - 1) / stacked_layers)
      
    Decoding (WrappingVariant):
      // Decoding in WrappingVariant: read the window output and decode them
      window_read_time = window_size_mib / rig_storage_read_mibs
      window_read_time_parallel = window_read_time / rig_storage_parallelization
      decoding_time = encoding_window_time + window_read_time
      // Note: this is an upperbound, it can be lower in theory min(time, disk access)
      decoding_time_parallel = encoding_window_time_parallel + window_read_time_parallel

    Decoding (Wrapping):
      // Decoding in Wrapping: read all the window outputs and decode them
      window_read_time = sector_size_mib / rig_storage_read_mibs
      window_read_time_parallel = window_read_time / rig_storage_parallelization
      decoding_time = encoding_time + window_read_time
      // Note: this is an upperbound, it can be lower in theory min(time, disk access)
      decoding_time_parallel = encoding_time_parallel + window_read_time_parallel

  Cost:
    describe(porep_cost, "Cost of a seal proof", dollar)
    describe(porep_snark_cost, "Cost of a snark proof", dollar)
    describe(porep_commit_cost, "Cost of generating merkle trees", dollar)
    describe(porep_decoding_cost, "Cost of decoding a sector", dollar)
    describe(porep_encoding_cost, "Cost of decoding a sector", dollar)

    porep_commit_cost = porep_commit_time * rig_cpu_core_cost_secs
    porep_encoding_cost = encoding_time * rig_cpu_core_cost_secs
    porep_snark_cost = porep_snark_time * rig_gpu_cost_secs

    porep_cost = porep_commit_cost + porep_encoding_cost + porep_snark_cost
    porep_decoding_cost = decoding_time * rig_cpu_core_cost_secs

  SNARK:
    describe(porep_snark_constraints, "Constraints for a PoRep", constraints)
    describe(porep_snark_partitions, "Number of partitions of a PoRep proof")
    describe(porep_snark_time, "Time to generate a PoRep SNARK proof", secs)
    describe(porep_snark_time_mins, "Time to generate a PoRep SNARK proof", mins)
    describe(porep_snark_time_parallel, "Time to compute a PoRep snark in parallel")
    describe(porep_snark_size, "Size of a PoRep proof", bytes)
    describe(porep_snark_size_kib, "Size of a PoRep proof", kib)
    describe(porep_snark_partition_constraints, "Number of constraints for a PoRep partition")

    describe(kdf_constraints, "Constraints of KDF per node parent")
    describe(labeling_proof, "Constraints to check labeling of a single node")
    describe(labeling_proofs, "Constraints to check labeling of a window")
    describe(window_labeling_proof, "Constraints to check correct labeling for all windows")
    describe(data_inclusion, "Constraints to check a single inclusion in CommD")
    describe(data_inclusions, "Constraints to check all the inclusions required in a window in CommD")
    describe(window_data_inclusions,  "Constraints to check all the inclusions required for all windows in CommD")
    describe(replica_inclusion, "Constraints to check a single inclusion in CommR")
    describe(replica_inclusions, "Constraints to check all the inclusions required in a window in CommR")
    describe(window_replica_inclusions,  "Constraints to check all the inclusions required for all windows in CommR")
    describe(window_inclusion, "Constraints to check a single inclusion proof in a window")
    describe(window_inclusions, "Constraints to check all the required inclusion proofs in a window")
    describe(column_leaf, "Constraints to check a single leaf generation from a column in a window")
    describe(column_leaves, "Constraints to check all the leaves generation required in a window")
    describe(window_column_leaves, "Constraints to check all the leaves generation required for all windows")
    describe(porep_snark_partition_time, "Time to compute a single snark partion", secs)
    describe(porep_commd_inclusions_constraints, "Constraints for CommD inclusions (PoRep)")
    describe(porep_commr_inclusions_constraints, "Constraints for CommR inclusions (PoRep)")
    describe(porep_commc_inclusions_constraints, "Constraints for CommC inclusions (PoRep)")
    describe(porep_commc_leaves_constraints, "Constraints for CommC leaves (PoRep)")
    describe(porep_labeling_proofs_constraints, "Constraints for labeling proofs (PoRep)")

    // D
    // Data layer committed with merkle tree
    data_inclusion = merkle_tree_datahash_constraints * log2(nodes)
    data_inclusions = data_inclusion * porep_challenges
    window_data_inclusions = data_inclusion * porep_challenges * windows

    // C
    window_inclusion = merkle_tree_hash_constraints * log2(window_nodes)
    window_inclusions =  window_inclusion * porep_challenges * (graph_parents + 1)
    column_leaf = column_leaf_hash_constraints * stacked_layers
    column_leaves = column_leaf * porep_challenges * (graph_parents + 1)
    window_column_leaves = column_leaves * windows
    labeling_proof = kdf_constraints * graph_parents
    labeling_proofs = labeling_proof * stacked_layers * porep_challenges
    window_labeling_proof = labeling_proofs * windows

    // Q and R
    replica_inclusion = merkle_tree_hash_constraints * commr_tree_depth 
    replica_inclusions = replica_inclusion * porep_challenges
    window_replica_inclusions = replica_inclusions * windows

    porep_snark_partitions = porep_snark_constraints / porep_snark_partition_constraints
    porep_snark_size = porep_snark_partitions * snark_size
    porep_snark_size_kib = porep_snark_size / b_in_kib
    porep_snark_time = snark_constraint_time * porep_snark_constraints
    porep_snark_time_mins = porep_snark_time / secs_in_min 
    porep_snark_partition_time = snark_constraint_time * porep_snark_constraints
    // Parallelization calculation: it must take at least the time for a single snark
    // the remaining can be done in parallel
    // Similar in spirit to:
    // min(porep_snark_partition_time, porep_snark_time / snarrk_parallelization)
    porep_snark_time_parallel = porep_snark_partition_time + (porep_snark_time - porep_snark_partition_time) / (rig_snark_parallelization)

    // Constraints (WindowEncoding):
    //   porep_snark_constraints = window_inclusions + window_column_leaves + window_labeling_proof + window_data_inclusions + window_replica_inclusions*2


    Constraints (StackedReplicas):
      porep_commd_inclusions_constraints = data_inclusions
      porep_commr_inclusions_constraints = replica_inclusions
      porep_commc_inclusions_constraints = window_inclusions
      porep_commc_leaves_constraints = window_column_leaves
      porep_labeling_proofs_constraints = window_labeling_proof

      porep_snark_constraints = porep_commc_leaves_constraints + porep_commc_inclusions_constraints + porep_commr_inclusions_constraints + porep_commd_inclusions_constraints + porep_labeling_proofs_constraints

      // porep_snark_constraints = window_inclusions + window_column_leaves + window_labeling_proof + data_inclusions + replica_inclusions*2

    // TODO: add wrapper constraints
    Constraints (Wrapping):
      porep_snark_constraints = window_inclusions + window_column_leaves + window_labeling_proof + data_inclusions + replica_inclusions*2

    Constraints (WrappingVariant):
      porep_snark_constraints = window_inclusions + window_column_leaves + window_labeling_proof + data_inclusions + replica_inclusions*2

    // Column comitting data or replica. 

SDR:
  describe(sdr_layers_a, "SDR layers computation a")
  describe(sdr_layers_b, "SDR layers computation b")
  describe(sdr_layers_c, "SDR layers computation c")
  describe(sdr_delta, "Fraction of pebbled nodes encoded incorrectly")

ProofOfReplication (StackedSDRParameters):
  sdr_eps = spacegap

  stacked_layers = sdr_layers_a + 1 // TODO max(layers_a, layers_b) + 1
  sdr_layers_a = (0.68 - sdr_eps + 4 * sdr_delta) / (0.12 - sdr_delta) + 2
  sdr_layers_b = (log2(1 / (3 * (sdr_eps - 4 * sdr_delta)))) + 0.24 / (0.12 - sdr_delta) + 1
  sdr_layers_c = (log2(2 / (1 + 3 * (sdr_eps - 5 * sdr_delta)))) + (0.57 - sdr_eps + 3 * sdr_delta) / (0.12 - sdr_delta) + 1

  porep_challenges = -porep_lambda / (log2(1 - sdr_delta))
  post_challenges = -post_lambda / (log2(1 - sdr_eps/2))

  drg_e = 0.8
  drg_d = 0.2 // 1/4
  drg_parents = 6
  expander_parents = 8

ProofOfReplication (StackedChungParameters):
  describe(chung_a, "Chung A")
  describe(chung_c, "Chung C")
  describe(chung_delta, "Chung chung_delta")
  describe(chung_sigma, "Chung sigma")

  stacked_layers = (log2(chung_psi / chung_a) / log2(chung_c)) + 1
  porep_challenges = -porep_lambda / (log2(1 - chung_delta))
  post_challenges = -post_lambda / (log2(chung_sigma))

  drg_parents = 0

  chung_alpha = regeneration_fraction * cost_amax
  chung_eps = spacegap

  chung_a = chung_eps - (chung_c * chung_delta  / (chung_c -1)) + (((chung_c - 1) * (chung_sigma - 1))/ chung_c)
  chung_c = chung_beta / chung_alpha

  chung_psi = chung_alpha
  chung_sigma = 1 - chung_eps + chung_delta + 0.0001

  Chung: ./bin/chung_spec.py
    chung_beta = extern(chung_alpha, expander_parents)
    // chung_alpha = extern(chung_beta, expander_parents)
    // expander_parents = extern(chung_alpha, chung_beta)

EC:
  describe(ec_e, "Number of winning miners per epoch on expectation")

Filecoin:
  describe(filecoin_block_time, "Time between two blocks", secs)
  describe(blocks_in_a_year, "Number of blocks in a year")

  filecoin_block_time = block_time
  blocks_in_a_year = (secs_in_year / filecoin_block_time) * ec_e

  Onboarding:
    describe(onboard_gib_time, "Time to onboard 1 GiB in the network", secs)
    describe(onboard_tib_time, "Time to onboard 1 TiB in the network", secs)
    describe(onboard_tib_time_days, "Time to onboard 1 TiB in the network", days)
    onboard_gib_time = (1 / sector_size_gib) * porep_time
    onboard_tib_time = onboard_gib_time * gib_in_tib
    onboard_tib_time_days = onboard_tib_time / secs_in_day 

  Storage:
    describe(filecoin_sectors, "Number of sectors in the Filecoin DSN")
    describe(filecoin_miners, "Number of Storage Miners")
    describe(filecoin_storage_capacity, "Amount of storage that the Filecoin DSN can support", bytes)
    describe(filecoin_storage_capacity_gib, "Amount of storage that the Filecoin DSN can support", gib)
    describe(filecoin_storage_capacity_eib, "Amount of storage that the Filecoin DSN can support", eib)

    filecoin_sectors = filecoin_storage_capacity_gib / sector_size_gib
    filecoin_storage_capacity_eib = filecoin_storage_capacity / b_in_eib
    filecoin_storage_capacity_gib = filecoin_storage_capacity / b_in_gib

  Chain:
    describe(chain_size_year, "Size of one year worth of blocks", bytes)
    describe(chain_size_year_gib, "Size of one year worth of blocks", kib)
    describe(block_size, "Size of a block", bytes)
    describe(block_size_kib, "Size of a block", bytes)
    describe(proofs_block_fraction, "Fraction of the block dedicated to proofs")

    chain_size_year = block_size * blocks_in_a_year
    chain_size_year_gib = chain_size_year / b_in_gib
    block_size_kib = block_size / b_in_kib

    // Note this is simplistic and accounts for no framing
    block_size = (proofs_per_block / proofs_block_fraction) 
    not_proofs_per_block = block_size - proofs_per_block

    proofs_per_block_kib = proofs_per_block / b_in_kib
    not_proofs_per_block_kib = not_proofs_per_block / b_in_kib

  OnChainProofs: 
    describe(seals_per_year, "Number of PoRep proofs per year")
    describe(posts_per_year, "Number of PoSt proofs per year")
    describe(filecoin_reseals_per_year, "Number of times a sector is re-sealed in a year")
    describe(seals_per_sector_per_year, "Number of seal proofs per sector per year")
    describe(posts_per_sector_per_year, "Number of post proofs per sector per year")
    describe(seals_size_per_year, "Size of seal proofs per year", bytes)
    describe(posts_size_per_year, "Size of post proofs per year", bytes)
    describe(seals_size_per_block, "Size of seal proofs per block", bytes)
    describe(posts_size_per_block, "Size of post proofs per block", bytes)
    describe(posts_size_per_block_kib, "Size of post proofs per block", kib)
    describe(seals_size_per_block_kib, "Size of seal proofs per block", kib)
    describe(proofs_per_block, "Size of proofs per block", bytes)
    describe(proofs_per_block_kib, "Size of proofs per block", kib)
    describe(not_proofs_per_block, "Size of non-proof data per block", bytes)
    describe(not_proofs_per_block_kib, "Size of non-proof data per block", kib)

    seals_per_sector_per_year = filecoin_reseals_per_year + 1
    seals_per_year = filecoin_sectors * seals_per_sector_per_year
    posts_per_year = filecoin_sectors * posts_per_sector_per_year 

    seals_size_per_year = seals_per_year * porep_proof_size
    posts_size_per_year = posts_per_year * post_proof_size

    seals_size_per_block = seals_size_per_year / blocks_in_a_year
    posts_size_per_block = posts_size_per_year / blocks_in_a_year

    proofs_per_block = seals_size_per_block + posts_size_per_block
    proofs_per_block_kib = proofs_per_block / b_in_kib
    seals_size_per_block_kib = seals_size_per_block / b_in_kib
    posts_size_per_block_kib = posts_size_per_block / b_in_kib

    PoSt (ElectionWithFallbackPoSt):
      describe(fposts_per_sector_per_year, "Number of fallback post proofs per sector per year")
      describe(eposts_per_sector_per_year, "Number of election post proofs per sector per year")
      describe(fallback_ratio, "Fraction of fallback PoSts")
      describe(fallback_period, "Time between last proof submitted and a required fallback PoSt", secs)
      describe(fallback_period_days, "Time between last proof submitted and a required fallback PoSt", secs)

      fallback_period_days = fallback_period / secs_in_day
      posts_per_sector_per_year = eposts_per_sector_per_year + fposts_per_sector_per_year

      // TODO: model single ticket vs multiple tickets
      ElectionPoSt:
        eposts_per_sector_per_year = blocks_in_a_year / filecoin_sectors

      FallbackPoSt:
        fposts_per_sector_per_year = (secs_in_year / fallback_period) * fallback_ratio

  Mining:
    ElectionPoSt:
      describe(epost_challenged_sectors, "Number of challenged sectors during Election PoSt")
      describe(epost_challenged_sectors_fraction, "Fraction of challenged sectors during Election PoSt")
      describe(epost_data_access, "Time to access data for Election PoSt", secs)
      describe(epost_data_access_parallel, "Time to access data for Election PoSt in parallel", secs)
      describe(epost_time, "Time to generate an Election PoSt", secs)
      describe(epost_time_parallel, "Time to generate an Election PoSt in parallel", secs)
      describe(epost_inclusions_time, "Time to generate all inclusions in Election PoSt", secs)
      describe(epost_inclusions_time_parallel, "Time to generate all the inclusions in Election PoSt in parallel", secs)
      describe(epost_leaves_read, "Time to read the challenged leaves nodes during EPoSt", secs)
      describe(epost_leaves_read_parallel, "Time to read the challenged leaves nodes during EPoSt in parallel", secs)
      describe(epost_mtree_read, "Time to read Merkle tree nodes during EPoSt", secs)
      describe(epost_mtree_read_parallel, "Time to read the Merkle tree during EPoSt in parallel", secs)

      epost_challenged_sectors = rig_sectors_min * epost_challenged_sectors_fraction
      epost_leaves_read = post_leaves_read * epost_challenged_sectors
      epost_mtree_read = post_mtree_read
      epost_data_access = epost_leaves_read + epost_mtree_read
      epost_data_access_parallel = epost_data_access / rig_storage_parallelization  
      epost_mtree_read_parallel = epost_mtree_read / rig_storage_parallelization  
      epost_leaves_read_parallel = epost_leaves_read / rig_storage_parallelization  
      epost_inclusions_time = post_inclusions_time * epost_challenged_sectors
      epost_inclusions_time_parallel = epost_inclusions_time / rig_cores

      epost_time = epost_data_access + post_ticket_gen + epost_inclusions_time + post_snark_time
      epost_time_parallel = epost_data_access_parallel + post_ticket_gen + epost_inclusions_time_parallel + post_snark_time_parallel

      describe(epost_cost, "Cost for generating a EPoSt", dollar)
      describe(epost_snark_cost, "Cost for generating a EPoSt SNARK", dollar)
      describe(epost_inclusions_cost, "Cost for generating a EPoSt mtrees", dollar)

      epost_inclusions_cost = epost_inclusions_time * rig_cpu_core_cost_secs
      epost_snark_cost = post_snark_time * rig_gpu_cost_secs
      // TODO: missing disk access
      epost_cost = epost_inclusions_cost + epost_snark_cost

Timing:
  describe(malicious_regen_time, "Time for a malicious adversary to regenerate a sector", secs)
  describe(time_amax, "Factor of how much slower than the blocktime the malicious encoding should take")

Timing (TimingAssumption):
  RegenAttack (StackedSDRParameters):
  malicious_regen_time = (encoding_time_asic * drg_d) / (stacked_layers)
  malicious_regen_time = block_time * time_amax

Rationality:
  describe(breakeven_nodes, "number of nodes / sector / challenge period after which it's cheaper to store the entire sector, than re-encode those")
  describe(block_time, "Time between two PoSt proofs", secs)
  describe(challenge_periods_per_month, "Number of challenge periods in a month")
  describe(regeneration_fraction, "Fraction of nodes of a sector for which re-encoding them every `block_time` costs as much as storing them")

  breakeven_nodes = ((cost_gib_per_month *  sector_size_gib) / ((1 - porep_soundness) * cost_node_encoding * challenge_periods_per_month))
  challenge_periods_per_month = secs_in_month / block_time
  regeneration_fraction = breakeven_nodes / nodes

  Costs:
    Compute:
      describe(cost_node_encoding, "Cost of encoding a single node")
      describe(nodes_per_second, "Nodes encoded per second using the fastest hardware")
      describe(nodes_per_year, "Nodes encoded per year using the fastest hardware")
      describe(hash_gb_per_second, "GB that can be hashed per second")
      describe(hash_gib_per_second, "GiB that can be hashed per second")

      hash_gib_per_second = hash_gb_per_second * gb_to_gib
      nodes_per_second = hash_gib_per_second / kdf_hash_size_gib
      nodes_per_year = nodes_per_second * secs_in_year 
      cost_node_encoding = rig_malicious_cost_per_year / nodes_per_year 

    Storage:
      // TODO(important): add the hardware cost from first principle
      describe(cost_gb_per_month, "Cost of storing 1GB per month")
      describe(cost_gib_per_month, "Cost of storing 1GiB per month")
      describe(cost_storage_gibs, "Cost of storing 1GiB per second")
      describe(cost_storage_sector_block_time, "Cost of storage for `block_time`")
      describe(extra_storage_time, "Time for which an honest user must be storing a file beyond the challenge period")

      cost_gib_per_month = cost_gb_per_month * gb_to_gib
      cost_storage_gibs = cost_gib_per_month / secs_in_month 
      cost_storage_sector_block_time = cost_storage_gibs * (sector_size_gib * (block_time + extra_storage_time))
